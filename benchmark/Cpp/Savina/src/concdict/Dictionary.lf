/**
 * Concurrency benchmark from the Savina benchmark suite. Intended
 * to measure reader-writer concurrency support.
 * See https://shamsimam.github.io/papers/2014-agere-savina.pdf.
 * 
 * To break the causality loop the dictionary reactor contains a logical
 * action.
 * 
 * @author Hannes Klein
 */

target Cpp {
    build-type : RelWithDebInfo,
    logging: "warn"
};

import BenchmarkRunner from "../BenchmarkRunner.lf";

public preamble {=
    enum class AccessType {
      Read,
      Write
    };
    
    struct Message {
      AccessType type;
      int key;
      int value;
    };
=}

reactor Manager(numWorkers: size_t{20}) {
    
    public preamble {=
        #include "reactor-cpp/logging.hh"
    =}
    
    state numWorkersTerminated: size_t{0};
    
    input start: void;
    output finished: void;
    
    output doWork: void;
    input[numWorkers] workerFinished: void;
    
    reaction(start) -> doWork {=
        // reset local state
        numWorkersTerminated = 0;
        
        // start execution
        doWork.set();
    =}
    
    reaction(workerFinished) -> finished {=
        for (const auto& fin : workerFinished) {
            if (fin.is_present()) {
                numWorkersTerminated++;
            }
        }
        if(numWorkersTerminated == numWorkers) {
            reactor::log::Info() << "All " << numWorkersTerminated << " workers terminated.";
            finished.set();
        }
    =}
}

reactor DictionaryImpl(numWorkers: size_t{20}) {
    
    public preamble {=
        #include "reactor-cpp/logging.hh"
    =}
    
    state dataMap: std::map<int,int>;
    state answersToSend: std::vector<int>;
    
    logical action sendAnswers: void;

    input[numWorkers] request: Message;
    output[numWorkers] response: int; 
    input reset: void;
    
    reaction(reset) {=
        // reset local state
        dataMap.clear();
        answersToSend.clear();
        answersToSend.resize(numWorkers, -1);
    =}
    
    reaction(sendAnswers) -> response {=
        for(size_t i{0}; i < numWorkers; i++) {
            if(answersToSend[i] >= 0) {
                response[i].set(answersToSend[i]);
                answersToSend[i] = -1;
            }
        }
    =}
    
    reaction(request) -> sendAnswers {=
        // The order of messages to read is relevant, it effectively
        // assigns priorities to the workers.
        for(size_t i{0}; i < numWorkers; i++) {
            if(request[i].is_present()) {
                auto msg = request[i].get();
                
                if(msg->type == AccessType::Write) {  
                    dataMap.emplace(msg->key, msg->value);
                    // Savina sends ResultMsg always independently if adding (key,value)
                    // to the map was successful.
                    answersToSend[i] = msg->value;
                    reactor::log::Info() << "Worker " << i << " writes " << msg->value << " to key " << msg->key;
                } else if(msg->type == AccessType::Read) {
                    // read the value. If the key is not yet present, this will add it and initialize the value to 0.
                    // This does not match 100% the Savina implementation which returns null if the key is not present,
                    // but should do a similar job.
                    int value = dataMap[msg->key];
                    answersToSend[i] = value;
                    reactor::log::Info() << "Worker " << i << " reads key " << msg->key << "; response is " << value; 
                }
            }
        }
        
        sendAnswers.schedule();
    =}
}

reactor Worker(bank_index: size_t{0}, numMessagesPerWorker: size_t{10000}, writePercentage: int{10}) {
    
    public preamble {=
        #include <random>
    =}
    
    state messageCount: size_t{0};
    state random: std::minstd_rand;
    
    input doWork: void;
    output finished: void;
    
    input dictResponse: int;
    output dictRequest: Message;
    
    reaction(doWork, dictResponse) -> dictRequest, finished {=
        if (doWork.is_present()) {
            //reset local state
            random.seed(bank_index + numMessagesPerWorker + writePercentage);
            messageCount = 0;
        }
        
        messageCount += 1;
        if(messageCount <= numMessagesPerWorker) {
            int anInt = static_cast<int>(random()) % 100;
            if(anInt < writePercentage) {
                dictRequest.set(Message{AccessType::Write, static_cast<int>(random()), static_cast<int>(random())});
            } else {
                dictRequest.set(Message{AccessType::Read, static_cast<int>(random()), 0});
            }
        } else {
            finished.set();
        }
    =}
}

main reactor (numIterations: size_t{12}, numMessagesPerWorker: size_t{10000}, writePercentage: int{10}, numWorkers: size_t{20}) {
    
    manager = new Manager(numWorkers=numWorkers);
    runner = new BenchmarkRunner(numIterations=numIterations);
    
    runner.outIterationStart -> manager.start;
    manager.finished -> runner.inIterationFinish;
    
    reaction(startup) -> runner.inStart {=
        printBenchmarkInfo("DictionaryReactorLFCppBenchmark");
        printArgs("numIterations", numIterations, "numMessagesPerWorker", numMessagesPerWorker,
                  "writePercentage", writePercentage, "numWorkers", numWorkers);
        printSystemInfo();
        runner.inStart.set();
    =}
    
    dict = new DictionaryImpl(numWorkers=numWorkers);
    workers = new[numWorkers] Worker(numMessagesPerWorker=numMessagesPerWorker, writePercentage=writePercentage);
    
    dict.response -> workers.dictResponse;
    workers.dictRequest -> dict.request;
    workers.finished -> manager.workerFinished;
    (manager.doWork)+ -> workers.doWork, dict.reset;
}